- 배운 것
gradient가 죽지 않게 해야한다. 1. 초기값(weight, bias 모두 중요!) 잘 주기 2. 학습률을 크게 잡지 않기
leaky_relu와 relu간 계산량 차이는 크다. 효과에 별 차이가 없다면 relu를 쓰자.

- 실험을 통해 알게된 사실
레이어를 더 많이 쌓으면 오히려 나빠진다.( 어떤 입력에 대해서도 같은 결과가 나오는 현상 = 어떤 지점에서 학습이 멈춤 = relu의 모든 출력을 0으로 해버리는 레어어가 생김) 3개 이하로 쌓아도 잘 동작한다.
마지막 레이어의 activation을 sigmoid로 하면 학습이 좀 더 빠르고 loss가 더 많이 준다. relu로 해도 동작은 한다.
weight 0, bias 0이면 grad가 죽는다. relu가 조기에 다 블럭되서. weight가 0이 아니면 초반에 grad가 너무 커져서 0으로 해야하고, bias는 0이 아니여야만 bias 크기만큼의 적절한 grad가 생긴다!

- Todo


- next Todo
encoder/decoder weight share
convolutional ae
batch norm.